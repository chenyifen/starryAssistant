/*
 * Dicio SenseVoice Input Device - Refactored
 * åŸºäºSenseVoiceå¤šè¯­è¨€ASRçš„è¯­éŸ³è¾“å…¥è®¾å¤‡å®ç°
 * 
 * é‡æ„è¯´æ˜ï¼š
 * - å‚è€ƒSherpaOnnxSimulateStreamingAsrå®˜æ–¹demoçš„è®¾è®¡æ¨¡å¼
 * - ä½¿ç”¨çŠ¶æ€é©±åŠ¨è€ŒéJobç®¡ç†
 * - ç®€åŒ–åç¨‹ç”Ÿå‘½å‘¨æœŸç®¡ç†
 * - ä¿®å¤æ—¶åºæ··ä¹±å’Œåç¨‹å–æ¶ˆé—®é¢˜
 */

package org.stypox.dicio.io.input.sensevoice

import android.content.Context
import android.media.AudioFormat
import android.media.AudioRecord
import android.media.MediaRecorder
import android.util.Log
import com.k2fsa.sherpa.onnx.Vad
import kotlinx.coroutines.*
import kotlinx.coroutines.channels.Channel
import kotlinx.coroutines.flow.MutableStateFlow
import kotlinx.coroutines.flow.StateFlow
import kotlinx.coroutines.flow.asStateFlow
import org.stypox.dicio.di.LocaleManager
import org.stypox.dicio.io.input.InputEvent
import org.stypox.dicio.io.input.SttInputDevice
import org.stypox.dicio.io.input.SttState
import org.stypox.dicio.util.DebugLogger
import java.util.concurrent.atomic.AtomicBoolean

/**
 * SenseVoiceè¯­éŸ³è¾“å…¥è®¾å¤‡ - å•ä¾‹æ¨¡å¼
 * 
 * è®¾è®¡åŸåˆ™ï¼š
 * 1. çŠ¶æ€é©±åŠ¨ï¼šä½¿ç”¨ isRecording æ ‡å¿—æ§åˆ¶æµç¨‹ï¼Œè€ŒéJobå¼•ç”¨
 * 2. ä¸¤ä¸ªç‹¬ç«‹åç¨‹ï¼šéŸ³é¢‘é‡‡é›†(IO) + éŸ³é¢‘å¤„ç†(Default)
 * 3. Channelé€šä¿¡ï¼šåç¨‹é—´é€šè¿‡Channelä¼ é€’éŸ³é¢‘æ•°æ®
 * 4. è‡ªåŠ¨æ¸…ç†ï¼šèµ„æºåœ¨finallyå—ä¸­è‡ªåŠ¨é‡Šæ”¾
 */
class SenseVoiceInputDevice private constructor(
    private val appContext: Context,
    private val localeManager: LocaleManager,
) : SttInputDevice {

    companion object {
        private const val TAG = "SenseVoiceInputDevice"
        
        // éŸ³é¢‘é…ç½® (ä¸å®˜æ–¹demoä¿æŒä¸€è‡´)
        private const val SAMPLE_RATE = 16000
        private const val CHANNEL_CONFIG = AudioFormat.CHANNEL_IN_MONO
        private const val AUDIO_FORMAT = AudioFormat.ENCODING_PCM_16BIT
        private const val AUDIO_SOURCE = MediaRecorder.AudioSource.MIC
        
        // VADå’Œè¯†åˆ«å‚æ•°
        private const val VAD_WINDOW_SIZE = 512                    // VADçª—å£å¤§å° (32ms @ 16kHz)
        private const val RECOGNITION_INTERVAL_MS = 200L           // å®æ—¶è¯†åˆ«é—´éš” (ä¸demoä¸€è‡´)
        private const val SPEECH_TIMEOUT_MS = 6000L                // é™éŸ³è¶…æ—¶ (3ç§’)
        private const val MAX_RECORDING_DURATION_MS = 30000L       // æœ€å¤§å½•åˆ¶æ—¶é•¿ (30ç§’)
        private const val MIN_SPEECH_DURATION_MS = 500L            // æœ€çŸ­æœ‰æ•ˆè¯­éŸ³

        // å•ä¾‹å®ä¾‹
        @Volatile
        private var INSTANCE: SenseVoiceInputDevice? = null

        fun getInstance(appContext: Context, localeManager: LocaleManager): SenseVoiceInputDevice {
            return INSTANCE ?: synchronized(this) {
                INSTANCE ?: SenseVoiceInputDevice(appContext, localeManager).also { 
                    INSTANCE = it
                    Log.d(TAG, "ğŸ—ï¸ åˆ›å»ºSenseVoiceInputDeviceå•ä¾‹å®ä¾‹")
                }
            }
        }

        fun resetInstance() {
            synchronized(this) {
                INSTANCE?.let { instance ->
                    Log.d(TAG, "ğŸ”„ é‡ç½®SenseVoiceInputDeviceå•ä¾‹å®ä¾‹")
                    CoroutineScope(Dispatchers.Default).launch {
                        instance.destroy()
                    }
                }
                INSTANCE = null
            }
        }
    }

    // ========== ç¡¬ä»¶èµ„æº ==========
    private var senseVoiceRecognizer: SenseVoiceRecognizer? = null
    private var vad: Vad? = null
    private var audioRecord: AudioRecord? = null
    
    // ========== çŠ¶æ€ç®¡ç† ==========
    private val isInitialized = AtomicBoolean(false)
    private val isRecording = AtomicBoolean(false)  // ä¸»æ§åˆ¶æ ‡å¿—
    
    private val _uiState = MutableStateFlow<SttState>(SttState.NotInitialized)
    override val uiState: StateFlow<SttState> = _uiState.asStateFlow()
    
    // ========== é€šä¿¡Channel ==========
    private var samplesChannel = Channel<FloatArray>(capacity = Channel.UNLIMITED)
    
    // ========== åç¨‹ä½œç”¨åŸŸ ==========
    private var scope = CoroutineScope(Dispatchers.Default + SupervisorJob())
    
    // ========== äº‹ä»¶ç›‘å¬ ==========
    private var eventListener: ((InputEvent) -> Unit)? = null
    
    // ========== éŸ³é¢‘ç¼“å†² ==========
    // ä½¿ç”¨ä¸¤ä¸ªç¼“å†²åŒºï¼šä¸€ä¸ªç”¨äºVADæ£€æµ‹ï¼Œä¸€ä¸ªç”¨äºç´¯ç§¯å·²æ£€æµ‹åˆ°çš„è¯­éŸ³
    private val vadBuffer = ArrayDeque<Float>(VAD_WINDOW_SIZE * 2)  // VADå¤„ç†çš„æ»‘åŠ¨çª—å£
    private val speechBuffer = arrayListOf<Float>()  // æ£€æµ‹åˆ°è¯­éŸ³åç´¯ç§¯çš„éŸ³é¢‘æ•°æ®
    
    // ========== VADçŠ¶æ€ ==========
    private var isSpeechDetected = false
    private var speechStartTime = 0L
    private var lastRecognitionTime = 0L
    private var lastSpeechTime = 0L  // æœ€åä¸€æ¬¡æ£€æµ‹åˆ°è¯­éŸ³çš„æ—¶é—´
    private var lastEnergyLogTime = 0L  // èƒ½é‡æ—¥å¿—æ—¶é—´æˆ³
    private var lastText = ""
    private var added = false  // å‚è€ƒdemoçš„ç»“æœç®¡ç†

    init {
        Log.d(TAG, "ğŸ—ï¸ [INIT] SenseVoiceInputDeviceæ„é€ å‡½æ•°å¼€å§‹")
        Log.d(TAG, "ğŸ¤ SenseVoiceè¾“å…¥è®¾å¤‡æ­£åœ¨åˆå§‹åŒ–...")
        Log.d(TAG, "ğŸš€ [INIT] å¯åŠ¨åç¨‹åˆå§‹åŒ–ç»„ä»¶")
        scope.launch {
            Log.d(TAG, "ğŸ”„ [COROUTINE] initializeComponents()åç¨‹å¼€å§‹æ‰§è¡Œ")
            initializeComponents()
            Log.d(TAG, "âœ… [COROUTINE] initializeComponents()åç¨‹æ‰§è¡Œå®Œæˆ")
        }
        Log.d(TAG, "âœ… [INIT] SenseVoiceInputDeviceæ„é€ å‡½æ•°å®Œæˆ")
    }
    
    /**
     * åˆå§‹åŒ–è¯†åˆ«å™¨å’ŒVAD
     */
    private suspend fun initializeComponents() {
        Log.d(TAG, "ğŸ”§ å¼€å§‹åˆå§‹åŒ–ç»„ä»¶...")
        _uiState.value = SttState.Loading(thenStartListening = false)
        
        try {
            // æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§
            if (!SenseVoiceModelManager.isModelAvailable(appContext)) {
                Log.e(TAG, "âŒ SenseVoiceæ¨¡å‹ä¸å¯ç”¨")
                _uiState.value = SttState.ErrorLoading(Exception("SenseVoiceæ¨¡å‹ä¸å¯ç”¨"))
                return
            }
            
            // åˆ›å»ºè¯†åˆ«å™¨
            senseVoiceRecognizer = SenseVoiceRecognizer.create(appContext)
            if (senseVoiceRecognizer == null) {
                Log.e(TAG, "âŒ è¯†åˆ«å™¨åˆ›å»ºå¤±è´¥")
                _uiState.value = SttState.ErrorLoading(Exception("è¯†åˆ«å™¨åˆ›å»ºå¤±è´¥"))
                return
            }
            
            // åˆå§‹åŒ–VAD
            if (VadModelManager.isVadModelAvailable(appContext)) {
                val vadConfig = VadModelManager.createVadConfig(appContext)
                val modelPaths = VadModelManager.getVadModelPaths(appContext)
                if (vadConfig != null && modelPaths != null) {
                    try {
                        // ğŸ”§ ä¿®å¤ï¼šæ ¹æ®æ¨¡å‹æ¥æºé€‰æ‹©æ­£ç¡®çš„æ„é€ å‡½æ•°
                        // Vadæ„é€ å‡½æ•°ç­¾å: Vad(assetManager: AssetManager?, config: VadModelConfig)
                        vad = if (modelPaths.isFromAssets) {
                            Log.d(TAG, "ğŸ”§ ä»AssetsåŠ è½½VADæ¨¡å‹")
                            Vad(appContext.assets, vadConfig)
                        } else {
                            Log.d(TAG, "ğŸ”§ ä»æ–‡ä»¶ç³»ç»ŸåŠ è½½VADæ¨¡å‹: ${modelPaths.modelPath}")
                            Vad(null, vadConfig)  // assetManagerä¼ nullï¼Œä½¿ç”¨é…ç½®ä¸­çš„æ–‡ä»¶è·¯å¾„
                        }
                        Log.d(TAG, "âœ… VADåˆå§‹åŒ–æˆåŠŸ")
                        Log.d(TAG, "ğŸ“Š ${VadModelManager.getVadModelInfo(appContext)}")
                    } catch (e: Exception) {
                        Log.e(TAG, "âŒ VADåˆå§‹åŒ–å¤±è´¥ï¼Œå›é€€åˆ°èƒ½é‡æ£€æµ‹", e)
                        vad = null
                    }
                } else {
                    Log.w(TAG, "âš ï¸ VADé…ç½®åˆ›å»ºå¤±è´¥ï¼Œä½¿ç”¨èƒ½é‡æ£€æµ‹")
                    vad = null
                }
            } else {
                Log.w(TAG, "âš ï¸ VADæ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨èƒ½é‡æ£€æµ‹")
                vad = null
            }
            
            isInitialized.set(true)
            _uiState.value = SttState.Loaded
            
            Log.d(TAG, "âœ… åˆå§‹åŒ–å®Œæˆ")
            Log.d(TAG, "ğŸ“Š ${SenseVoiceModelManager.getModelInfo(appContext)}")
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ åˆå§‹åŒ–å¼‚å¸¸", e)
            _uiState.value = SttState.ErrorLoading(e)
        }
    }
    
    /**
     * å¯åŠ¨è¯­éŸ³è¯†åˆ«
     * å‚è€ƒå®˜æ–¹demoçš„ç®€æ´è®¾è®¡
     */
    override fun tryLoad(thenStartListeningEventListener: ((InputEvent) -> Unit)?): Boolean {
        Log.d(TAG, "ğŸš€ å¯åŠ¨è¯­éŸ³è¯†åˆ«")
        
        // æ£€æŸ¥åˆå§‹åŒ–çŠ¶æ€
        if (!isInitialized.get() || senseVoiceRecognizer == null) {
            Log.e(TAG, "âŒ è¯†åˆ«å™¨æœªåˆå§‹åŒ–")
            return false
        }
        
        // ä½¿ç”¨CASç¡®ä¿åŸå­æ€§æ“ä½œ
        if (!isRecording.compareAndSet(false, true)) {
            Log.w(TAG, "âš ï¸ å·²åœ¨å½•åˆ¶ä¸­")
            return true
        }
        
        // ç¡®ä¿åç¨‹ä½œç”¨åŸŸå¯ç”¨
        if (!scope.isActive) {
            scope = CoroutineScope(Dispatchers.Default + SupervisorJob())
            Log.d(TAG, "ğŸ”„ é‡æ–°åˆ›å»ºåç¨‹ä½œç”¨åŸŸ")
        }
        
        // ä¿å­˜äº‹ä»¶ç›‘å¬å™¨
        this.eventListener = thenStartListeningEventListener
        
        // é‡ç½®çŠ¶æ€
        resetRecordingState()
        
        // æ›´æ–°UIçŠ¶æ€
        _uiState.value = SttState.Listening
        
        // å¯åŠ¨éŸ³é¢‘é‡‡é›†åç¨‹ (IO Dispatcher)
        scope.launch(Dispatchers.IO) {
            recordAudio()
        }
        
        // å¯åŠ¨éŸ³é¢‘å¤„ç†åç¨‹ (Default Dispatcher)
        scope.launch(Dispatchers.Default) {
            processAudio()
        }
        
        return true
    }
    
    /**
     * ç‚¹å‡»äº‹ä»¶å¤„ç†
     */
    override fun onClick(eventListener: (InputEvent) -> Unit) {
        Log.d(TAG, "ğŸ–±ï¸ ç‚¹å‡»äº‹ä»¶")
        
        if (isRecording.get()) {
            stopListening()
        } else {
            tryLoad(eventListener)
        }
    }
    
    /**
     * åœæ­¢è¯­éŸ³è¯†åˆ«
     * ç®€å•è®¾ç½®æ ‡å¿—ï¼Œåç¨‹ä¼šè‡ªç„¶ç»“æŸ
     */
    override fun stopListening() {
        if (!isRecording.get()) {
            return
        }
        
        Log.d(TAG, "ğŸ›‘ åœæ­¢è¯­éŸ³è¯†åˆ«")
        isRecording.set(false)
        
        // æ³¨æ„ï¼šä¸éœ€è¦æ‰‹åŠ¨å–æ¶ˆåç¨‹æˆ–æ¸…ç†èµ„æº
        // åç¨‹ä¼šé€šè¿‡ isRecording æ ‡å¿—è‡ªç„¶ç»“æŸ
        // èµ„æºä¼šåœ¨ finally å—ä¸­è‡ªåŠ¨æ¸…ç†
    }
    
    /**
     * é”€æ¯è®¾å¤‡
     */
    override suspend fun destroy() {
        Log.d(TAG, "ğŸ§¹ é”€æ¯è®¾å¤‡...")
        
        try {
            stopListening()
            
            samplesChannel.close()
            
            senseVoiceRecognizer?.release()
            senseVoiceRecognizer = null
            
            vad?.release()
            vad = null
            
            eventListener = null
            isInitialized.set(false)
            _uiState.value = SttState.NotInitialized
            
            Log.d(TAG, "âœ… è®¾å¤‡å·²é”€æ¯")
        } catch (e: Exception) {
            Log.e(TAG, "âŒ é”€æ¯å¤±è´¥", e)
        }
    }
    
    /**
     * é‡ç½®å½•åˆ¶çŠ¶æ€
     */
    private fun resetRecordingState() {
        vadBuffer.clear()
        speechBuffer.clear()
        isSpeechDetected = false
        speechStartTime = 0L
        lastRecognitionTime = 0L
        lastSpeechTime = 0L
        lastEnergyLogTime = 0L
        lastText = ""
        added = false
        vad?.reset()
        
        // é‡æ–°åˆ›å»ºChannel
        samplesChannel.close()
        samplesChannel = Channel(capacity = Channel.UNLIMITED)
        
        Log.d(TAG, "ğŸ”„ çŠ¶æ€å·²é‡ç½®")
    }
    
    /**
     * éŸ³é¢‘é‡‡é›†åç¨‹ - å‚è€ƒå®˜æ–¹demo
     * è¿è¡Œåœ¨ IO Dispatcher
     */
    private suspend fun recordAudio() = withContext(Dispatchers.IO) {
        try {
            Log.d(TAG, "ğŸµ å¯åŠ¨éŸ³é¢‘é‡‡é›†")
            
            // åˆ›å»ºAudioRecord
            val bufferSize = AudioRecord.getMinBufferSize(
                SAMPLE_RATE,
                CHANNEL_CONFIG,
                AUDIO_FORMAT
            )
            
            audioRecord = AudioRecord(
                AUDIO_SOURCE,
                SAMPLE_RATE,
                CHANNEL_CONFIG,
                AUDIO_FORMAT,
                bufferSize * 2
            )
            
            if (audioRecord?.state != AudioRecord.STATE_INITIALIZED) {
                Log.e(TAG, "âŒ AudioRecordåˆå§‹åŒ–å¤±è´¥")
                isRecording.set(false)
                return@withContext
            }
            
            audioRecord?.startRecording()
            Log.d(TAG, "âœ… å½•åˆ¶å·²å¯åŠ¨")
            
            // éŸ³é¢‘é‡‡é›†ç¼“å†²åŒº (100ms)
            val interval = 0.1
            val frameSize = (interval * SAMPLE_RATE).toInt()
            val buffer = ShortArray(frameSize)
            
            // æŒç»­é‡‡é›†ç›´åˆ°åœæ­¢æ ‡å¿—
            var totalSamplesRead = 0
            while (isRecording.get()) {
                val ret = audioRecord?.read(buffer, 0, buffer.size) ?: -1
                
                when {
                    ret > 0 -> {
                        // è½¬æ¢ä¸ºFloatå¹¶å½’ä¸€åŒ–
                        val samples = FloatArray(ret) { i ->
                            buffer[i].toFloat() / 32768.0f
                        }
                        
                        // æ·»åŠ éŸ³é¢‘æ•°æ®è¯Šæ–­ (å‰å‡ æ¬¡æˆ–å®šæœŸ)
                        totalSamplesRead++
                        if (totalSamplesRead <= 3 || totalSamplesRead % 50 == 0) {
                            val maxSample = samples.maxOrNull() ?: 0f
                            val minSample = samples.minOrNull() ?: 0f
                            val avgSample = samples.average()
                            Log.v(TAG, "ğŸ“Š éŸ³é¢‘æ•°æ®#$totalSamplesRead: size=$ret, max=${"%.4f".format(maxSample)}, min=${"%.4f".format(minSample)}, avg=${"%.6f".format(avgSample)}")
                        }
                        
                        samplesChannel.send(samples)
                    }
                    ret == 0 -> {
                        delay(1)
                    }
                    ret < 0 -> {
                        Log.e(TAG, "âŒ éŸ³é¢‘è¯»å–é”™è¯¯: $ret")
                        break
                    }
                }
            }
            
            // å‘é€ç»“æŸä¿¡å·
            samplesChannel.send(FloatArray(0))
            Log.d(TAG, "ğŸ éŸ³é¢‘é‡‡é›†ç»“æŸ")
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ éŸ³é¢‘é‡‡é›†å¼‚å¸¸", e)
        } finally {
            // æ¸…ç†AudioRecord
            cleanupAudioRecord()
        }
    }
    
    /**
     * æ¸…ç†AudioRecordèµ„æº
     */
    private fun cleanupAudioRecord() {
        audioRecord?.let {
            try {
                if (it.state == AudioRecord.STATE_INITIALIZED) {
                    it.stop()
                }
                it.release()
            } catch (e: Exception) {
                Log.w(TAG, "æ¸…ç†AudioRecordå¤±è´¥", e)
            }
        }
        audioRecord = null
        Log.d(TAG, "âœ… AudioRecordå·²æ¸…ç†")
    }
    
    /**
     * éŸ³é¢‘å¤„ç†åç¨‹ - å‚è€ƒå®˜æ–¹demo
     * è¿è¡Œåœ¨ Default Dispatcher
     */
    private suspend fun processAudio() = withContext(Dispatchers.Default) {
        try {
            Log.d(TAG, "ğŸ”„ å¯åŠ¨éŸ³é¢‘å¤„ç†")
            
            val startTime = System.currentTimeMillis()
            
            while (isRecording.get()) {
                for (samples in samplesChannel) {
                    // æ£€æŸ¥ç»“æŸä¿¡å·
                    if (samples.isEmpty()) {
                        Log.d(TAG, "ğŸ“¥ æ”¶åˆ°ç»“æŸä¿¡å·")
                        break
                    }
                    
                    // å¤„ç†æ–°çš„éŸ³é¢‘æ ·æœ¬
                    val hasSpeech = processNewSamples(samples)
                    
                    // æ£€æŸ¥æœ€å¤§å½•åˆ¶æ—¶é•¿
                    val elapsed = System.currentTimeMillis() - startTime
                    if (elapsed > MAX_RECORDING_DURATION_MS) {
                        Log.d(TAG, "â° è¾¾åˆ°æœ€å¤§å½•åˆ¶æ—¶é—´")
                        break
                    }
                    
                    // å®æ—¶è¯†åˆ«ï¼ˆåªæœ‰æ£€æµ‹åˆ°è¯­éŸ³åæ‰æ‰§è¡Œï¼‰
                    if (isSpeechDetected) {
                        performPartialRecognition()
                    }
                    
                    // æ£€æŸ¥é™éŸ³è¶…æ—¶
                    if (isSpeechDetected && !hasSpeech) {
                        val currentTime = System.currentTimeMillis()
                        val silenceDuration = currentTime - lastSpeechTime
                        if (silenceDuration > SPEECH_TIMEOUT_MS) {
                            Log.d(TAG, "ğŸ”‡ æ£€æµ‹åˆ°é™éŸ³è¶…æ—¶ (${silenceDuration}ms)")
                            break
                        }
                    }
                }
                
                // é€€å‡ºforå¾ªç¯ï¼Œè¯´æ˜éœ€è¦åœæ­¢
                break
            }
            
            Log.d(TAG, "ğŸ¯ å¼€å§‹æœ€ç»ˆè¯†åˆ«")
            performFinalRecognition()
            
        } catch (e: CancellationException) {
            Log.d(TAG, "ğŸ›‘ éŸ³é¢‘å¤„ç†è¢«å–æ¶ˆ")
            throw e
        } catch (e: Exception) {
            Log.e(TAG, "âŒ éŸ³é¢‘å¤„ç†å¼‚å¸¸", e)
            withContext(Dispatchers.Main) {
                eventListener?.invoke(InputEvent.Error(e))
            }
        } finally {
            isRecording.set(false)
            _uiState.value = SttState.Loaded
            Log.d(TAG, "ğŸ éŸ³é¢‘å¤„ç†ç»“æŸ")
        }
    }
    
    /**
     * å¤„ç†æ–°çš„éŸ³é¢‘æ ·æœ¬å¹¶è¿›è¡ŒVADæ£€æµ‹
     * è¿”å›trueè¡¨ç¤ºå½“å‰å¸§åŒ…å«è¯­éŸ³
     */
    private fun processNewSamples(samples: FloatArray): Boolean {
        var hasSpeech = false
        val currentTime = System.currentTimeMillis()
        
        // å°†æ–°æ ·æœ¬æ·»åŠ åˆ°VADç¼“å†²åŒº
        for (sample in samples) {
            vadBuffer.addLast(sample)
            
            // å¦‚æœå·²ç»æ£€æµ‹åˆ°è¯­éŸ³ï¼Œä¹Ÿæ·»åŠ åˆ°è¯­éŸ³ç¼“å†²åŒº
            if (isSpeechDetected) {
                speechBuffer.add(sample)
            }
        }
        
        // å½“VADç¼“å†²åŒºè¾¾åˆ°çª—å£å¤§å°æ—¶è¿›è¡Œæ£€æµ‹
        while (vadBuffer.size >= VAD_WINDOW_SIZE) {
            // å–å‡ºä¸€ä¸ªçª—å£çš„æ•°æ®è¿›è¡ŒVADæ£€æµ‹
            val vadWindow = FloatArray(VAD_WINDOW_SIZE) { i -> vadBuffer.elementAt(i) }
            
            // ä½¿ç”¨VADæˆ–èƒ½é‡æ£€æµ‹åˆ¤æ–­æ˜¯å¦æœ‰è¯­éŸ³
            val speechDetected = if (vad != null) {
                vad!!.acceptWaveform(vadWindow)
                vad!!.isSpeechDetected()
            } else {
                detectSpeechByEnergy(vadWindow)
            }
            
            if (speechDetected) {
                hasSpeech = true
                lastSpeechTime = currentTime
                
                // å¦‚æœä¹‹å‰æœªæ£€æµ‹åˆ°è¯­éŸ³ï¼Œç°åœ¨æ£€æµ‹åˆ°äº†
                if (!isSpeechDetected) {
                    isSpeechDetected = true
                    speechStartTime = currentTime
                    // å°†VADç¼“å†²åŒºä¸­çš„æ‰€æœ‰æ•°æ®ä¹ŸåŠ å…¥åˆ°è¯­éŸ³ç¼“å†²åŒºï¼ˆåŒ…æ‹¬è¯­éŸ³å¼€å§‹å‰çš„ä¸€å°æ®µï¼‰
                    for (sample in vadBuffer) {
                        speechBuffer.add(sample)
                    }
                    DebugLogger.logRecognition(TAG, "ğŸ™ï¸ æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹")
                }
            }
            
            // ç§»é™¤å·²å¤„ç†çš„æ ·æœ¬ï¼ˆæ»‘åŠ¨çª—å£ï¼Œæ­¥é•¿ä¸ºçª—å£å¤§å°çš„1/4ä»¥æé«˜æ£€æµ‹çµæ•åº¦ï¼‰
            repeat(VAD_WINDOW_SIZE / 4) {
                if (vadBuffer.isNotEmpty()) {
                    vadBuffer.removeFirst()
                }
            }
        }
        
        return hasSpeech
    }
    
    /**
     * ç®€å•èƒ½é‡æ£€æµ‹ (VADé™çº§æ–¹æ¡ˆ)
     * æé«˜é˜ˆå€¼ä»¥å‡å°‘è¯¯æŠ¥
     */
    private fun detectSpeechByEnergy(samples: FloatArray): Boolean {
        if (samples.isEmpty()) return false
        
        var sum = 0.0
        for (sample in samples) {
            sum += (sample * sample).toDouble()
        }
        val rms = kotlin.math.sqrt(sum / samples.size)
        
        // æé«˜é˜ˆå€¼åˆ°0.01ï¼Œé¿å…å¤ªå¤šå™ªéŸ³è¢«è¯¯æ£€æµ‹ä¸ºè¯­éŸ³
        // 0.003å¤ªä½äº†ï¼Œä¼šæŠŠèƒŒæ™¯å™ªéŸ³ä¹Ÿå½“ä½œè¯­éŸ³
        val threshold = 0.01
        val detected = rms > threshold
        
        // æ·»åŠ è°ƒè¯•æ—¥å¿— - å¸®åŠ©è¯Šæ–­é—®é¢˜
        if (detected && !isSpeechDetected) {
            DebugLogger.logRecognition(TAG, "ğŸ”Š èƒ½é‡æ£€æµ‹è§¦å‘: RMS=${"%.6f".format(rms)} > threshold=${"%.6f".format(threshold)}")
        } else if (System.currentTimeMillis() - lastEnergyLogTime > 2000) {
            // æ¯2ç§’è®°å½•ä¸€æ¬¡èƒ½é‡å€¼ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
            Log.v(TAG, "ğŸ”Š éŸ³é¢‘èƒ½é‡: RMS=${"%.6f".format(rms)}, é˜ˆå€¼=${"%.6f".format(threshold)}, å·²æ£€æµ‹=$isSpeechDetected")
            lastEnergyLogTime = System.currentTimeMillis()
        }
        
        return detected
    }
    
    /**
     * å®æ—¶éƒ¨åˆ†è¯†åˆ« - å‚è€ƒå®˜æ–¹demo
     */
    private suspend fun performPartialRecognition() {
        if (!isSpeechDetected) {
            Log.v(TAG, "â­ï¸ è·³è¿‡å®æ—¶è¯†åˆ« - æœªæ£€æµ‹åˆ°è¯­éŸ³")
            return
        }
        
        val currentTime = System.currentTimeMillis()
        val elapsed = currentTime - lastRecognitionTime
        
        // æ¯200msæ‰§è¡Œä¸€æ¬¡è¯†åˆ«ï¼Œä¸”è¯­éŸ³æ•°æ®è¦è¶³å¤Ÿé•¿ï¼ˆè‡³å°‘0.5ç§’ï¼‰
        if (elapsed >= RECOGNITION_INTERVAL_MS && speechBuffer.size >= SAMPLE_RATE / 2) {
            val recognizer = senseVoiceRecognizer ?: return
            
            DebugLogger.logRecognition(TAG, "ğŸ”„ å¼€å§‹å®æ—¶è¯†åˆ« (${speechBuffer.size}æ ·æœ¬)")
            
            // ä½¿ç”¨ç´¯ç§¯çš„è¯­éŸ³æ•°æ®è¿›è¡Œè¯†åˆ«
            val audioData = speechBuffer.toFloatArray()
            val text = recognizer.recognize(audioData)
            
            lastText = text
            lastRecognitionTime = currentTime
            
            if (text.isNotBlank()) {
                // å‚è€ƒdemoçš„ç»“æœç®¡ç†
                withContext(Dispatchers.Main) {
                    if (!added) {
                        eventListener?.invoke(InputEvent.Partial(text))
                        added = true
                        DebugLogger.logRecognition(TAG, "ğŸ“¤ é¦–æ¬¡å‘é€: $text")
                    } else {
                        eventListener?.invoke(InputEvent.Partial(text))
                        DebugLogger.logRecognition(TAG, "ğŸ“¤ æ›´æ–°ç»“æœ: $text")
                    }
                }
            }
        }
    }
    
    /**
     * æœ€ç»ˆè¯†åˆ« - ä½¿ç”¨ NonCancellable ç¡®ä¿ä¸è¢«ä¸­æ–­
     */
    private suspend fun performFinalRecognition() = withContext(NonCancellable) {
        try {
            val recognizer = senseVoiceRecognizer ?: return@withContext
            
            // æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆéŸ³é¢‘
            if (speechBuffer.isEmpty() || !isSpeechDetected) {
                Log.d(TAG, "âš ï¸ æ²¡æœ‰æœ‰æ•ˆè¯­éŸ³æ•°æ®")
                withContext(Dispatchers.Main) {
                    eventListener?.invoke(InputEvent.None)
                }
                return@withContext
            }
            
            // æ£€æŸ¥æ—¶é•¿
            val duration = System.currentTimeMillis() - speechStartTime
            if (duration < MIN_SPEECH_DURATION_MS) {
                Log.d(TAG, "âš ï¸ è¯­éŸ³æ—¶é•¿å¤ªçŸ­: ${duration}ms")
                withContext(Dispatchers.Main) {
                    eventListener?.invoke(InputEvent.None)
                }
                return@withContext
            }
            
            val audioDurationSec = speechBuffer.size.toFloat() / SAMPLE_RATE
            Log.d(TAG, "ğŸš€ æ‰§è¡Œæœ€ç»ˆè¯†åˆ«ï¼ŒéŸ³é¢‘: ${speechBuffer.size}æ ·æœ¬ (${String.format("%.2f", audioDurationSec)}ç§’)")
            
            // æ‰§è¡Œè¯†åˆ«
            val audioData = speechBuffer.toFloatArray()
            val text = recognizer.recognize(audioData)
            
            DebugLogger.logRecognition(TAG, "âœ… æœ€ç»ˆç»“æœ: \"$text\"")
            
            // å‘é€ç»“æœ
            withContext(Dispatchers.Main) {
                if (text.isNotBlank()) {
                    // å‚è€ƒdemoï¼šå¦‚æœæœ‰éƒ¨åˆ†ç»“æœï¼Œæ›´æ–°å®ƒï¼›å¦åˆ™æ·»åŠ æ–°ç»“æœ
                    if (lastText.isNotBlank() && added) {
                        eventListener?.invoke(InputEvent.Final(listOf(Pair(text, 1.0f))))
                    } else {
                        eventListener?.invoke(InputEvent.Final(listOf(Pair(text, 1.0f))))
                    }
                    added = false
                } else {
                    eventListener?.invoke(InputEvent.None)
                }
            }
            
        } catch (e: Exception) {
            Log.e(TAG, "âŒ æœ€ç»ˆè¯†åˆ«å¼‚å¸¸", e)
            withContext(Dispatchers.Main) {
                eventListener?.invoke(InputEvent.Error(e))
            }
        }
    }
    
    /**
     * è·å–è®¾å¤‡ä¿¡æ¯
     */
    fun getDeviceInfo(): String {
        val recognizerInfo = senseVoiceRecognizer?.getInfo() ?: "æœªåˆå§‹åŒ–"
        val bufferSize = speechBuffer.size
        val vadBufferSize = vadBuffer.size
        val isActive = isRecording.get()
        return "SenseVoiceDevice($recognizerInfo, è¯­éŸ³ç¼“å†²:${bufferSize}æ ·æœ¬, VADç¼“å†²:${vadBufferSize}æ ·æœ¬, æ´»è·ƒ:$isActive)"
    }
}
