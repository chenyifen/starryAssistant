#!/usr/bin/env python3
"""
éŸ©è¯­å”¤é†’è¯æ‰¹é‡æ•°æ®é›†ç”Ÿæˆå™¨
ä¸€æ¬¡æ€§ç”Ÿæˆå¤§é‡å¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®ï¼ŒåŒ…æ‹¬è¿œåœºè¯­éŸ³æ”¯æŒ

åŠŸèƒ½ç‰¹ç‚¹:
- æ‰¹é‡ç”Ÿæˆå¤§è§„æ¨¡æ•°æ®é›†
- å¤šç§TTSå¼•æ“å¹¶è¡Œå¤„ç†
- è‡ªåŠ¨æ•°æ®å¹³è¡¡å’Œè´¨é‡æ§åˆ¶
- è¿œåœºè¯­éŸ³æ¨¡æ‹Ÿ
- æ•°æ®å¢å¼ºå’Œå˜æ¢
- è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†åˆ’åˆ†

ä½¿ç”¨æ–¹æ³•:
    python scripts/batch_generate_korean_dataset.py --total_samples 5000
    python scripts/batch_generate_korean_dataset.py --large_dataset --far_field
    python scripts/batch_generate_korean_dataset.py --custom --positive 2000 --negative 8000
"""

import os
import sys
import argparse
import logging
import asyncio
import random
import time
from pathlib import Path
from typing import Dict, List, Tuple
import json
from concurrent.futures import ThreadPoolExecutor
import shutil

# å¯¼å…¥æˆ‘ä»¬çš„ç”Ÿæˆå™¨
sys.path.append(str(Path(__file__).parent))
from advanced_korean_tts_generator import AdvancedKoreanTTSGenerator

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('batch_korean_dataset.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class BatchKoreanDatasetGenerator:
    """æ‰¹é‡éŸ©è¯­æ•°æ®é›†ç”Ÿæˆå™¨"""
    
    def __init__(self, output_dir: str):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        self.train_dir = self.output_dir / "train"
        self.val_dir = self.output_dir / "val" 
        self.test_dir = self.output_dir / "test"
        self.raw_dir = self.output_dir / "raw"
        
        for dir_path in [self.train_dir, self.val_dir, self.test_dir, self.raw_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # æ•°æ®é›†é…ç½®
        self.dataset_configs = {
            'small': {'positive': 200, 'negative': 800, 'far_field': 100},
            'medium': {'positive': 1000, 'negative': 4000, 'far_field': 500},
            'large': {'positive': 3000, 'negative': 12000, 'far_field': 1500},
            'xlarge': {'positive': 5000, 'negative': 20000, 'far_field': 2500}
        }
        
        # æ•°æ®åˆ’åˆ†æ¯”ä¾‹ (è®­ç»ƒ:éªŒè¯:æµ‹è¯•)
        self.split_ratios = {'train': 0.7, 'val': 0.15, 'test': 0.15}
        
    def show_dataset_info(self):
        """æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯"""
        print("\n" + "="*80)
        print("ğŸ¯ éŸ©è¯­å”¤é†’è¯æ‰¹é‡æ•°æ®é›†ç”Ÿæˆå™¨")
        print("="*80)
        
        print(f"\nğŸ“Š é¢„è®¾æ•°æ®é›†é…ç½®:")
        for name, config in self.dataset_configs.items():
            total = config['positive'] + config['negative'] + config['far_field']
            print(f"  {name:8s}: æ­£æ ·æœ¬ {config['positive']:5d} + è´Ÿæ ·æœ¬ {config['negative']:5d} + è¿œåœº {config['far_field']:4d} = æ€»è®¡ {total:5d}")
        
        print(f"\nğŸ“ è¾“å‡ºç›®å½•ç»“æ„:")
        print(f"  {self.output_dir}/")
        print(f"  â”œâ”€â”€ raw/           # åŸå§‹ç”Ÿæˆæ•°æ®")
        print(f"  â”œâ”€â”€ train/         # è®­ç»ƒé›† (70%)")
        print(f"  â”œâ”€â”€ val/           # éªŒè¯é›† (15%)")
        print(f"  â”œâ”€â”€ test/          # æµ‹è¯•é›† (15%)")
        print(f"  â””â”€â”€ metadata.json  # æ•°æ®é›†å…ƒä¿¡æ¯")
        
        print("="*80)
    
    async def generate_dataset(self, config_name: str = None, custom_config: Dict = None, 
                             enable_far_field: bool = True) -> Dict:
        """ç”Ÿæˆå®Œæ•´æ•°æ®é›†"""
        
        # ç¡®å®šç”Ÿæˆé…ç½®
        if custom_config:
            config = custom_config
            config_name = "custom"
        elif config_name and config_name in self.dataset_configs:
            config = self.dataset_configs[config_name]
        else:
            config = self.dataset_configs['medium']
            config_name = "medium"
        
        logger.info(f"å¼€å§‹ç”Ÿæˆ {config_name} æ•°æ®é›†...")
        logger.info(f"é…ç½®: {config}")
        
        # åˆ›å»ºç”Ÿæˆå™¨
        generator = AdvancedKoreanTTSGenerator(str(self.raw_dir))
        
        start_time = time.time()
        
        try:
            # 1. ç”Ÿæˆæ­£æ ·æœ¬
            logger.info("ğŸ”Š ç”Ÿæˆæ­£æ ·æœ¬...")
            positive_count = await generator.generate_positive_samples_async(config['positive'])
            
            # 2. ç”Ÿæˆè´Ÿæ ·æœ¬
            logger.info("ğŸš« ç”Ÿæˆè´Ÿæ ·æœ¬...")
            negative_count = generator.generate_negative_samples(config['negative'])
            
            # 3. ç”Ÿæˆè¿œåœºæ ·æœ¬
            far_field_count = 0
            if enable_far_field and config.get('far_field', 0) > 0:
                logger.info("ğŸŒŠ ç”Ÿæˆè¿œåœºæ ·æœ¬...")
                far_field_count = generator.generate_far_field_samples(
                    generator.positive_dir, config['far_field']
                )
            
            # 4. æ•°æ®è´¨é‡æ£€æŸ¥
            logger.info("ğŸ” æ•°æ®è´¨é‡æ£€æŸ¥...")
            quality_stats = self._check_data_quality()
            
            # 5. æ•°æ®é›†åˆ’åˆ†
            logger.info("ğŸ“Š åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†...")
            split_stats = self._split_dataset()
            
            # 6. ç”Ÿæˆå…ƒæ•°æ®
            generation_time = time.time() - start_time
            metadata = self._generate_dataset_metadata(
                config_name, config, quality_stats, split_stats, generation_time
            )
            
            logger.info(f"âœ… æ•°æ®é›†ç”Ÿæˆå®Œæˆï¼è€—æ—¶: {generation_time:.1f}ç§’")
            
            return {
                'config_name': config_name,
                'config': config,
                'generated_counts': {
                    'positive': positive_count,
                    'negative': negative_count,
                    'far_field': far_field_count
                },
                'quality_stats': quality_stats,
                'split_stats': split_stats,
                'generation_time': generation_time,
                'metadata': metadata
            }
            
        except Exception as e:
            logger.error(f"æ•°æ®é›†ç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    def _check_data_quality(self) -> Dict:
        """æ£€æŸ¥æ•°æ®è´¨é‡"""
        logger.info("  æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶è´¨é‡...")
        
        stats = {
            'total_files': 0,
            'valid_files': 0,
            'invalid_files': 0,
            'duration_stats': {'min': float('inf'), 'max': 0, 'avg': 0},
            'size_stats': {'min': float('inf'), 'max': 0, 'avg': 0}
        }
        
        durations = []
        sizes = []
        
        # æ£€æŸ¥æ‰€æœ‰WAVæ–‡ä»¶
        for wav_file in self.raw_dir.rglob("*.wav"):
            stats['total_files'] += 1
            
            try:
                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                file_size = wav_file.stat().st_size
                sizes.append(file_size)
                
                # æ£€æŸ¥éŸ³é¢‘æ—¶é•¿ (ä½¿ç”¨librosa)
                import librosa
                duration = librosa.get_duration(filename=str(wav_file))
                durations.append(duration)
                
                # åŸºæœ¬è´¨é‡æ£€æŸ¥
                if 0.5 <= duration <= 3.0 and file_size > 1000:  # åˆç†çš„æ—¶é•¿å’Œå¤§å°
                    stats['valid_files'] += 1
                else:
                    stats['invalid_files'] += 1
                    logger.warning(f"è´¨é‡é—®é¢˜: {wav_file.name} (æ—¶é•¿: {duration:.2f}s, å¤§å°: {file_size})")
                
            except Exception as e:
                stats['invalid_files'] += 1
                logger.warning(f"æ— æ³•æ£€æŸ¥æ–‡ä»¶: {wav_file.name} - {e}")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if durations:
            stats['duration_stats'] = {
                'min': min(durations),
                'max': max(durations),
                'avg': sum(durations) / len(durations)
            }
        
        if sizes:
            stats['size_stats'] = {
                'min': min(sizes),
                'max': max(sizes),
                'avg': sum(sizes) / len(sizes)
            }
        
        logger.info(f"  è´¨é‡æ£€æŸ¥å®Œæˆ: {stats['valid_files']}/{stats['total_files']} æ–‡ä»¶æœ‰æ•ˆ")
        
        return stats
    
    def _split_dataset(self) -> Dict:
        """åˆ’åˆ†æ•°æ®é›†"""
        logger.info("  æ”¶é›†æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶...")
        
        # æ”¶é›†æ‰€æœ‰æ–‡ä»¶å’Œæ ‡ç­¾
        all_files = []
        
        # æ­£æ ·æœ¬
        for wav_file in (self.raw_dir / "positive").glob("*.wav"):
            all_files.append((wav_file, 1, 'positive'))
        
        # è´Ÿæ ·æœ¬
        for wav_file in (self.raw_dir / "negative").glob("*.wav"):
            all_files.append((wav_file, 0, 'negative'))
        
        # è¿œåœºæ ·æœ¬ (æ ‡è®°ä¸ºæ­£æ ·æœ¬)
        for wav_file in (self.raw_dir / "far_field").glob("*.wav"):
            all_files.append((wav_file, 1, 'far_field'))
        
        # æ‰“ä¹±æ•°æ®
        random.shuffle(all_files)
        
        # è®¡ç®—åˆ’åˆ†ç‚¹
        total_count = len(all_files)
        train_count = int(total_count * self.split_ratios['train'])
        val_count = int(total_count * self.split_ratios['val'])
        
        # åˆ’åˆ†æ•°æ®
        train_files = all_files[:train_count]
        val_files = all_files[train_count:train_count + val_count]
        test_files = all_files[train_count + val_count:]
        
        # å¤åˆ¶æ–‡ä»¶åˆ°å¯¹åº”ç›®å½•
        splits = {
            'train': (train_files, self.train_dir),
            'val': (val_files, self.val_dir),
            'test': (test_files, self.test_dir)
        }
        
        split_stats = {}
        
        for split_name, (files, target_dir) in splits.items():
            logger.info(f"  å¤åˆ¶ {len(files)} ä¸ªæ–‡ä»¶åˆ° {split_name} é›†...")
            
            # åˆ›å»ºå­ç›®å½•
            pos_dir = target_dir / "positive"
            neg_dir = target_dir / "negative"
            pos_dir.mkdir(exist_ok=True)
            neg_dir.mkdir(exist_ok=True)
            
            pos_count = neg_count = 0
            
            for i, (src_file, label, category) in enumerate(files):
                # ç”Ÿæˆæ–°æ–‡ä»¶å
                new_name = f"{split_name}_{category}_{i:05d}.wav"
                
                if label == 1:  # æ­£æ ·æœ¬
                    target_file = pos_dir / new_name
                    pos_count += 1
                else:  # è´Ÿæ ·æœ¬
                    target_file = neg_dir / new_name
                    neg_count += 1
                
                # å¤åˆ¶æ–‡ä»¶
                shutil.copy2(src_file, target_file)
            
            # ç”Ÿæˆæ–‡ä»¶åˆ—è¡¨
            self._create_file_list(target_dir, split_name)
            
            split_stats[split_name] = {
                'total': len(files),
                'positive': pos_count,
                'negative': neg_count
            }
        
        logger.info("  æ•°æ®é›†åˆ’åˆ†å®Œæˆ")
        return split_stats
    
    def _create_file_list(self, split_dir: Path, split_name: str):
        """åˆ›å»ºæ–‡ä»¶åˆ—è¡¨"""
        file_list_path = split_dir / f"{split_name}_files.txt"
        
        with open(file_list_path, 'w', encoding='utf-8') as f:
            # æ­£æ ·æœ¬
            for wav_file in sorted((split_dir / "positive").glob("*.wav")):
                rel_path = wav_file.relative_to(split_dir)
                f.write(f"{rel_path} 1\n")
            
            # è´Ÿæ ·æœ¬
            for wav_file in sorted((split_dir / "negative").glob("*.wav")):
                rel_path = wav_file.relative_to(split_dir)
                f.write(f"{rel_path} 0\n")
    
    def _generate_dataset_metadata(self, config_name: str, config: Dict, 
                                 quality_stats: Dict, split_stats: Dict, 
                                 generation_time: float) -> Dict:
        """ç”Ÿæˆæ•°æ®é›†å…ƒæ•°æ®"""
        metadata = {
            "dataset_info": {
                "name": f"korean_wake_word_{config_name}",
                "wake_word": "í•˜ì´ë„›ì§€",
                "language": "ko-KR",
                "generation_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "generation_duration_seconds": generation_time,
                "config_name": config_name,
                "config": config
            },
            "audio_specs": {
                "sample_rate": 16000,
                "channels": 1,
                "duration_seconds": 2.0,
                "format": "wav"
            },
            "dataset_stats": {
                "total_samples": quality_stats['total_files'],
                "valid_samples": quality_stats['valid_files'],
                "quality_stats": quality_stats,
                "split_stats": split_stats,
                "split_ratios": self.split_ratios
            },
            "usage": {
                "training": "Use train/ directory for model training",
                "validation": "Use val/ directory for hyperparameter tuning", 
                "testing": "Use test/ directory for final evaluation",
                "file_format": "Each split contains positive/ and negative/ subdirectories",
                "labels": "1 = wake word (positive), 0 = non-wake word (negative)"
            }
        }
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata_path = self.output_dir / "dataset_metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ•°æ®é›†å…ƒæ•°æ®å·²ä¿å­˜: {metadata_path}")
        return metadata
    
    def show_final_statistics(self, result: Dict):
        """æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "="*80)
        print("ğŸ‰ æ•°æ®é›†ç”Ÿæˆå®Œæˆï¼")
        print("="*80)
        
        config = result['config']
        generated = result['generated_counts']
        splits = result['split_stats']
        
        print(f"\nğŸ“Š ç”Ÿæˆç»Ÿè®¡:")
        print(f"  é…ç½®: {result['config_name']}")
        print(f"  âœ… æ­£æ ·æœ¬: {generated['positive']} ä¸ª")
        print(f"  âŒ è´Ÿæ ·æœ¬: {generated['negative']} ä¸ª")
        print(f"  ğŸŒŠ è¿œåœºæ ·æœ¬: {generated['far_field']} ä¸ª")
        print(f"  â±ï¸  ç”Ÿæˆæ—¶é—´: {result['generation_time']:.1f} ç§’")
        
        print(f"\nğŸ“ æ•°æ®é›†åˆ’åˆ†:")
        for split_name, stats in splits.items():
            print(f"  {split_name:5s}: {stats['total']:5d} ä¸ª (æ­£: {stats['positive']:4d}, è´Ÿ: {stats['negative']:4d})")
        
        print(f"\nğŸ¯ ä½¿ç”¨æ–¹æ³•:")
        print(f"  è®­ç»ƒ: {self.train_dir}")
        print(f"  éªŒè¯: {self.val_dir}")
        print(f"  æµ‹è¯•: {self.test_dir}")
        print(f"  å…ƒæ•°æ®: {self.output_dir}/dataset_metadata.json")
        
        print("="*80)

async def main():
    parser = argparse.ArgumentParser(description="éŸ©è¯­å”¤é†’è¯æ‰¹é‡æ•°æ®é›†ç”Ÿæˆå™¨")
    parser.add_argument("--output_dir", default="datasets/korean_wake_word",
                       help="è¾“å‡ºç›®å½•")
    
    # é¢„è®¾é…ç½®
    parser.add_argument("--small", action="store_true", help="ç”Ÿæˆå°å‹æ•°æ®é›† (1Kæ ·æœ¬)")
    parser.add_argument("--medium", action="store_true", help="ç”Ÿæˆä¸­å‹æ•°æ®é›† (5Kæ ·æœ¬)")
    parser.add_argument("--large", action="store_true", help="ç”Ÿæˆå¤§å‹æ•°æ®é›† (16Kæ ·æœ¬)")
    parser.add_argument("--xlarge", action="store_true", help="ç”Ÿæˆè¶…å¤§å‹æ•°æ®é›† (27Kæ ·æœ¬)")
    
    # è‡ªå®šä¹‰é…ç½®
    parser.add_argument("--custom", action="store_true", help="ä½¿ç”¨è‡ªå®šä¹‰é…ç½®")
    parser.add_argument("--positive", type=int, help="æ­£æ ·æœ¬æ•°é‡")
    parser.add_argument("--negative", type=int, help="è´Ÿæ ·æœ¬æ•°é‡")
    parser.add_argument("--far_field_samples", type=int, help="è¿œåœºæ ·æœ¬æ•°é‡")
    
    # é€‰é¡¹
    parser.add_argument("--no_far_field", action="store_true", help="ä¸ç”Ÿæˆè¿œåœºæ ·æœ¬")
    parser.add_argument("--show_info_only", action="store_true", help="ä»…æ˜¾ç¤ºä¿¡æ¯")
    
    args = parser.parse_args()
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = BatchKoreanDatasetGenerator(args.output_dir)
    generator.show_dataset_info()
    
    if args.show_info_only:
        return
    
    # ç¡®å®šé…ç½®
    config_name = None
    custom_config = None
    
    if args.custom:
        if not (args.positive and args.negative):
            print("âŒ è‡ªå®šä¹‰æ¨¡å¼éœ€è¦æŒ‡å®š --positive å’Œ --negative")
            return
        
        custom_config = {
            'positive': args.positive,
            'negative': args.negative,
            'far_field': args.far_field_samples or 0
        }
    else:
        # é€‰æ‹©é¢„è®¾é…ç½®
        if args.small:
            config_name = 'small'
        elif args.large:
            config_name = 'large'
        elif args.xlarge:
            config_name = 'xlarge'
        else:
            config_name = 'medium'  # é»˜è®¤
    
    enable_far_field = not args.no_far_field
    
    try:
        # ç”Ÿæˆæ•°æ®é›†
        result = await generator.generate_dataset(
            config_name=config_name,
            custom_config=custom_config,
            enable_far_field=enable_far_field
        )
        
        # æ˜¾ç¤ºç»“æœ
        generator.show_final_statistics(result)
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­ç”Ÿæˆè¿‡ç¨‹")
    except Exception as e:
        logger.error(f"æ‰¹é‡ç”Ÿæˆå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())
